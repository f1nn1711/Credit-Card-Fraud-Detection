{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "credit-card-fraud-detection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH6FYVZeNodh"
      },
      "source": [
        "# Fraudulent credit card transaction detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJzAK6FzOU7O"
      },
      "source": [
        "Firstly the required modules are imported: numpy is used for array functions; pandas is used for importing the dataset and dataframe handling; sklearn is used for the machine learning algorithms along with normalising the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3Gk0J_ZcB9O"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression as LogReg\n",
        "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3UDhlozPX2h"
      },
      "source": [
        "Then the dataset is imported from the git hub repository for this project but the original datset can be found [here.](https://www.kaggle.com/mlg-ulb/creditcardfraud) This dataset consists of ~285,000 transactions where each transaction has a time and an amount associated with it along with 28 features extracted using PCA (principal component analysis). In addition to this each transaction has a class of either 1 or 0, where 0 means it was a genuine transaction and a 1 means it was a fraudulent transaction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvFiy46cgUZ3"
      },
      "source": [
        "df = pd.read_csv(\"https://media.githubusercontent.com/media/f1nn1711/Credit-Card-Fraud-Detection/main/creditcard.csv\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8jpU-jCQzyw"
      },
      "source": [
        "This dataset is highly unbalanced as there is about 578 genuine transactions for ever fraudulent tranaction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_pgLKx5iTVt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed926d18-2d12-4555-a530-4498edb96d22"
      },
      "source": [
        "print(f\"Percent of transactions fraudulant: {round((len(df[df.Class == 1])/len(df))*100,5)}%\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percent of transactions fraudulant: 0.17275%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ID21DM2xSAtg"
      },
      "source": [
        "Each tranaction has an amount feature associated with it which can range from 0 to 25,691. This is a massive range for a single feature so it is important that these values are scaled down so the range of them is roughly in-line with the range of the other features. If these values aren't normalised this will result in the amount feature having a larger effect of the model during fitting but this feature might not be more important when it comes to predicting. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20hJMBqzn1w4"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "amount_values = df[\"Amount\"].values\n",
        "amount_values = amount_values.reshape(-1,1)\n",
        "\n",
        "df[\"Amount\"] = scaler.fit_transform(amount_values)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZUJ5UzbWMud"
      },
      "source": [
        "The dataframe is then split up in to 2 arrays where the array called X are the features and the array called y are the classes. The features are made up of the PCA values along with the normalised amount and the classes consist of either a 1 or a 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKjeRQ6Mqoms"
      },
      "source": [
        "X = df.drop([\"Time\",\"Class\"], axis=1).values\n",
        "y = df[\"Class\"].values"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwnq0v3HWqcE"
      },
      "source": [
        "The data is then split in to a training set and a testing set. Only the training set will be used to fit the models this is so the testing set can be used to evaluate the models performance on data it has never seen before. The `train_split` is a measure of how much of the data should be used for training, for example 0.85 means 85% will be used for training and 15% will be used for testing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8DVUgwYsuI5"
      },
      "source": [
        "train_split = 0.9\n",
        "\n",
        "split_index = round(len(df)*0.9)\n",
        "\n",
        "X_train = X[:split_index]\n",
        "y_train = y[:split_index]\n",
        "\n",
        "X_test = X[split_index:]\n",
        "y_test = y[split_index:]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uAeN0anZSDI"
      },
      "source": [
        "The first model to be trained is the logistic regression model. The logistic regression algorithm works is by fitting a logistic curve to the dataset. For example, instead of trying to predict credit card fraud let's imagine we trying to predict if a person has diabetes and the dataset we have consists of peoples blood glucose levels along with a 0 representing that they dont have diabetes and a 1 representing that they do. Then we would draw a 2D graph where the x-axis is the blood glucose level and the y-axis from 0 to 1 which represents if they do or do not have diabetes. All the elements in the dataset are plotted and a best fit logistic curve is then drawn for the data. Then when it comes to predicting is someone has diabetes their blood glucose levels can be calculated and plotted on the logistic curve, then the y-value for the logistic curve is calculated given the x values (the blood glucose level). The y-value will be a number between 0 and 1 and this can be thought of as the models confindence in whether a person has diabetes or not as, for example, if a person has a low glucose level then the y-value of the logistic curve might be 0.1 which would indicate there is a low chance that this person has diabetes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdWZRdq0z111",
        "outputId": "899c86cf-b146-4040-ef76-df7cffc7b442"
      },
      "source": [
        "logreg_model = LogReg()\n",
        "logreg_model.fit(X_train, y_train)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKYIF0KplNb6"
      },
      "source": [
        "The second model to be trained is the K-nearest-neighbour. The KNN works by seeing how a smaple is simlar to another sample. For example now let's say we are predicting if someone has diabetes using their weight and hours of exercise per week. We would have a dataset which contains the hours of exercise, weight and whether they have diabetes represented by a 1 if they do and a 0 if they do not. Then the dataset is plotted on a 2D graph with the hours of excersie per week on the x-axis and then their weight on the y-axis. When it comes to using this model to predict we plot the sample on the grpah. The class of the nearest neighbouring samples then will decide the classification of the new unknown sample. The K in KNN represents the number of neighbouring samples that will be looked at when classifying the new smaple. For example if the new sample's 5 nearest neighbour have the class of `0,1,0,1,1` then the new unknown sample will be classified as `1` as because 3 of the 5 nearest samples have that class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OY9Bb710V8N",
        "outputId": "adbbee0b-03fd-48ed-b7f8-03cef0d52b54"
      },
      "source": [
        "k=10\n",
        "knn_model = KNN(k)\n",
        "knn_model.fit(X_train, y_train)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
              "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
              "                     weights='uniform')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0HQSe_wCvEC"
      },
      "source": [
        "This `calculate_accuracy` function which calculates the accuracy of the model from the predicted vales and the actual values. This is done by comparing the predicted value and the actual value, if they are the same then the model was correct. At the end the number of correct predictions is divided by the total number of predictions to get the accuracy of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uL8LkwfX4s8s"
      },
      "source": [
        "def calculate_accuracy(pred, actu):\n",
        "    correct = 0\n",
        "\n",
        "    for a, p in zip(actu, pred):\n",
        "        if a == p:\n",
        "            correct += 1\n",
        "\n",
        "    return correct/pred.size"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6vKCMfEDPYG"
      },
      "source": [
        "The `apply_threshold` functions takes in a probability and a threshold and if the probability is less than the threshold the function will return 0 otherwise 1 will be returned. This function is then vectorized allowing it to be applied to an array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBKUP7BQeX9l"
      },
      "source": [
        "def apply_threshold(value, threshold):\n",
        "    if value < threshold:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "apply_threshold = np.vectorize(apply_threshold)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tB26fRvzF1zx"
      },
      "source": [
        "Once the logistic regression model has been fitted around the training dataset we classify the testing dataset with the model. The model returns an array for each element in the testing dataset. The first value in this array is the probabilty that the transaction is genuine and the second value is the probabilty that the transaction is fraudulent. So the first step is to filter the array so we only have the probabilty that the transaction is fraudulent. Then these probabilities need to be turned in to either a 1 or a 0, this is where the `apply_threshold` function is used. In this case the threshold is 0.1 so if the probabilty that the transaction is fraudulent is greater than 0.1 then it will be classified as fraudulent(1) otherwise it will be classified as genuine(0)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sMg2SGz-Tkk"
      },
      "source": [
        "logreg_prediction = logreg_model.predict_proba(X_test)#Return n length 2D array of where each element is [prob class_1, prob class_2]\n",
        "fraud_prob = np.reshape(np.delete(logreg_prediction, 0, axis=1), (-1))\n",
        "\n",
        "logreg_filtered_pred = apply_threshold(fraud_prob, 0.1)\n",
        "\n",
        "logreg_accuracy = calculate_accuracy(logreg_filtered_pred, y_test)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FT12CFe1RSAi"
      },
      "source": [
        "The same is then done for the K-nearest-neighbour model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wE3qk7zLDkpp"
      },
      "source": [
        "knn_prediction = knn_model.predict_proba(X_test)\n",
        "fraud_prob = np.reshape(np.delete(knn_prediction, 0, axis=1), (-1))\n",
        "\n",
        "knn_filtered_pred = apply_threshold(fraud_prob, 0.1)\n",
        "\n",
        "knn_accuracy = calculate_accuracy(knn_filtered_pred, y_test)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-XRELyyRogY"
      },
      "source": [
        "Then the accuracies of the model are printed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KryyzwAURxvq",
        "outputId": "40e9d6fd-ece8-492c-c59b-ab36dfe6a3b6"
      },
      "source": [
        "print(f\"Logistic regression accuracy: {logreg_accuracy}\")\n",
        "print(f\"K-nearest-neighbour accuracy: {knn_accuracy}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic regression accuracy: 0.9990519995786665\n",
            "K-nearest-neighbour accuracy: 0.9982093325374811\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMe2T-JaSZgf"
      },
      "source": [
        "The prediction results are then analysed to find out more about the model performs. This will analyse the number of: true negatives - this is when the model predicts it was genuine and it was genuine, true positives - this is when the model predicts it was fraudulent and it was fraudulent, false negatives - this is when the model predicts it was genuine and it was fraudulent, false positives - this is when the model predicts it was fraudulent and it was genuine. As before a `0` represents genuine transactions and a `1` represents a fraudulent transaction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNwuqISuFU7q"
      },
      "source": [
        "def analyse_predictions(pred, actu):\n",
        "    results = {\n",
        "        \"tn\" : 0,\n",
        "        \"tp\" : 0,\n",
        "        \"fn\" : 0,\n",
        "        \"fp\" : 0\n",
        "    }\n",
        "\n",
        "    for a, p in zip(actu, pred):\n",
        "        if a == 0:\n",
        "            if p == 0:\n",
        "                results[\"tn\"] += 1\n",
        "            else:\n",
        "                results[\"fn\"] += 1\n",
        "        else:\n",
        "            if p == 1:\n",
        "                results[\"tp\"] += 1\n",
        "            else:\n",
        "                results[\"fp\"] += 1\n",
        "    \n",
        "    return results"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re9ruAIAFyH9"
      },
      "source": [
        "The predictions are compared to the correct classifications using the `analyse_predictions` function and the results are printed out. In this case the results from the KNN model are being used however this could easily be changed to use the logistic regression model's results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LdaBsiOey7r",
        "outputId": "a8a0020d-35be-4343-bfcf-dc59882b8d3b"
      },
      "source": [
        "analysed_results = analyse_predictions(knn_filtered_pred, y_test)\n",
        "\n",
        "print(analysed_results)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'tn': 28414, 'tp': 16, 'fn': 45, 'fp': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpnWJ0u9GBbY"
      },
      "source": [
        "The first value to be printed is the percentage of how many times the model correctly predicted it was genuine and the second percentage is for how often the model predicts fraudulent and it was fraudulent. To calculate the percentage for the genuine transactions the number of times the model correctly predicted a genuine tranactions is divided by the total number of genuine transactions in the testing dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHqNsI7whH82",
        "outputId": "397041b1-6e8f-45c0-f05e-09f22b13f2ad"
      },
      "source": [
        "print(f\"{round((analysed_results['tn']/np.where(y_test == 0)[0].size)*100, 3)}% of genuine transaction identified.\")\n",
        "print(f\"{round((analysed_results['tp']/np.where(y_test == 1)[0].size)*100, 3)}% of fraudulent transaction identified.\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99.842% of genuine transaction identified.\n",
            "72.727% of fraudulent transaction identified.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}